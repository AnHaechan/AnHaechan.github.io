<!DOCTYPE html>

<html>
    <head>
        <meta charset="utf-8">
        <title> Personal webpage of Haechan An </title>
        <link rel="stylesheet" href="index.css">
    </head>

    <body>
        <header>
            <h1> Haechan An </h1>
        </header>

        <!--
        <nav>

        </nav>
        -->

        <main>
	    <p>
		    <img src="./img/haechan_25.jpg" width="10%"/>
	    </p>
            <p>
		    Currently I'm doing my masters program at KAIST Graduate School of Computing, under the advisement of
		    <a href = "https://cp.kaist.ac.kr/jeehoon.kang/"> Prof. Jeehoon Kang, of KAIST Concurrency & Parallelsim Lab</a>. </br>
                My interests are: AI systems, compilers, programming languages and machine learning.</br>
                I want to build a compiler, runtime and programming model that fill the gap between large AI models and heterogeneous hardwares, e.g., NPU and PIM. 
                <br>

                <br>
            </p>
	    <p>
		    <h3> single-page CV </h3>
		    <a href = "https://drive.google.com/file/d/1uPvAtXBY03NilHuWzWCt4Soj_MX8QLz9/view?usp=drive_link">CV.pdf</a>
	    </p>
            <p>
                <h3> Contact </h3>
                    <ul>
                        <li> Email : haechan.an_at_kaist.ac.kr </li>
                        <li> Github : AnHaechan </li>
                    </ul>
            </p>
	    <p>
		<h3> Research Topic: AI compilers for NPU-PIM heterogeneous systems </h3>
		<p>
		    Deep learning models, such as generative AI, are increasingly demanding more computation, memory and bandwidth.
	  		</br>
		    To efficiently handle those resource demands, (1) various specialized accelerators have been proposed
		    and (2) the model executions are being distributed to multiple hardwares.
			</br>
		    The benefit of using multiple heterogeneous accelerators can only be utilized by a software stack,
		    including specialized compiler and runtime, which bridge the high-level application and low-level hardwares.
			</br>
		    However, currently there are limited compiler support that can automatically generate efficient execution plans
		    for varying models, given multiple different accelerators including NPU(Neural Processing Unit) and PIM(Processing In Memory).
			</br>
		    My research goal is to design such <b>AI compilers for distributed & heterogeneous accelerators</b>.
		</p>
	    </p>
            <p>
                <h3> Experiences </h3>
                    <ul>
                        <li> 2021.01-2021.10, Undergraduate Internship, at <a href ="https://plrg.kaist.ac.kr/home">KAIST PLRG</a> </li>
                            	: worked on the instrumentation of JNI programs for bug-catching using dataflow analysis </br>
                            	: worked on finding Rust type patterns for shared mutable states in TockOS </br>
                        <li> 2022.01-2022.02, Startup </li>
                            	: developed data pipeline and web frontend </br>
                        <li> 2022.03-2022.06, Undergraduate Internship, at <a href ="https://prosys.kaist.ac.kr/">KAIST Prosys Lab</a> </li>
				: studied static program analysis and program synthesis; wrapped up it with <a href = https://drive.google.com/file/d/1NlnUOkdNGbU7ZY5Za-QfbXbk44ynreds/view?usp=sharing> my Bachelor's thesis </a></br>
                        <li> 2022.07-, Undergraduate Internship & <b>Master's Student</b>, at <a href ="https://cp.kaist.ac.kr/">KAIST Concurrency and Parallelism Lab</a> </li>
				: <b> worked on serving ML pipelines on NPU/GPU servers, with adpative scheduling (~5 mo) </b> </br>
				: worked on automatic program verification/formal proof automation with language models augmented with theorem provers (~1 yr) </br>
				: <b> currently working on AI compilers. Especially for compiling LLMs for NPU/PIM devices. (2024.03-) </b> </br>
                    </ul>
            </p>
	<p>
		<h3> External experiences </h3>
		<ul>
			<li> participated in <a href=https://news.skhynix.co.kr/post/kaist-aim-lecture>SK Hynix's workshop on AiM(Accelerator-in-Memory) theory & practice</a> </li>
			<li> participated in <a href=https://github.com/PyTorchKorea/pytorchcore-kr> Modulabs & Rebellions' PyTorch + NPU lab </a> </li>
		</ul>
	</p>
         
    </p>
    
        <footer>

        </footer>
    </body>
</html>
